# app_gradio_subject_predict.py
# -*- coding: utf-8 -*-
"""
subject_idë¡œ ê²€ìƒ‰:
  1) patients.csvì—ì„œ í™˜ì ì •ë³´ ì¡°íšŒ (subject_id, create, gender, anchor_age, anchor_year)
  2) ì €ì¥ëœ ëª¨ë¸(.pt) ë¡œë“œ â†’ ë™ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¬ì¶”ë¡ í•˜ì—¬ í‡´ì›ì¼ ì˜ˆì¸¡
     (í‘œì—ëŠ” pred_dischtime, los_pred_daysë§Œ í‘œì‹œ; los_pred_daysëŠ” ì†Œìˆ˜ì  3ìë¦¬)
"""

import os
import json
import torch
import gradio as gr
import pandas as pd
from torch.utils.data import DataLoader, Subset

# --- í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ ---
from utils.predictor.p_dataloader_5_3 import (
    HadmTableDatasetV3, collate_hadm_batch_v3, example_sources_config_v3
)
from architectures.predictor.predict_modelv2 import TableTransformerPredictor


# =========================
# ê¸°ë³¸ ê²½ë¡œ/ì˜µì…˜ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • ê°€ëŠ¥)
# =========================
PATIENTS_CSV = "./filtered/patients2.csv"

UNIFIED_CSV = "../dataset/summarized_with_readmit30_test.csv"
CKPT_PATH   = "./checkpoints_exp_final/highperf_best_exp_final.pt"
USE_EXAMPLE_SOURCES = True
SOURCES_JSON_PATH   = None
CACHE_DIR  = "./latent_cache_v2_exp_final"
BATCH_SIZE = 64

if torch.cuda.is_available():
    ENCODE_DEVICE = "cuda"
elif getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    ENCODE_DEVICE = "mps"
else:
    ENCODE_DEVICE = "cpu"


# =========================
# Utilities
# =========================
def resolve_device(device=None):
    if device is not None:
        return torch.device(device)
    if torch.cuda.is_available():
        return torch.device("cuda")
    if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def _pick_col(df: pd.DataFrame, candidates):
    """ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  í›„ë³´ ì¤‘ ì¡´ì¬í•˜ëŠ” ì²« ì»¬ëŸ¼ëª…ì„ ë°˜í™˜"""
    lower = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lower:
            return lower[cand.lower()]
    return None


# =========================
# 1) í™˜ì ì¡°íšŒ
# =========================
def lookup_patient(subject_id_str: str, patients_csv_path: str):
    if not subject_id_str or not subject_id_str.strip().isdigit():
        return "âŒ subject_idëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.", None
    sid = int(subject_id_str.strip())

    if not os.path.isfile(patients_csv_path):
        return f"âŒ patients.csv ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {patients_csv_path}", None

    df = pd.read_csv(patients_csv_path, low_memory=False)

    c_subj   = _pick_col(df, ["subject_id"])
    c_name   = _pick_col(df, ["create", "name", "patient_name"])  # 'create'ê°€ ê¸°ë³¸, ì—†ìœ¼ë©´ fallback
    c_gender = _pick_col(df, ["gender"])
    c_age    = _pick_col(df, ["anchor_age", "age"])
    c_year   = _pick_col(df, ["anchor_year", "birth_year"])

    for req, cname in {
        "subject_id": c_subj, "gender": c_gender, "anchor_age": c_age, "anchor_year": c_year
    }.items():
        if cname is None:
            return f"âŒ patients.csvì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {req}", None
    if c_name is None:
        # ì´ë¦„ ì»¬ëŸ¼ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
        df["__name__"] = "ê¹€ì§€í—Œ"
        c_name = "__name__"

    # ìˆ«ì ë³€í™˜ í›„ í•„í„°
    df[c_subj] = pd.to_numeric(df[c_subj], errors="coerce").astype("Int64")
    sel = df.loc[df[c_subj] == sid, [c_subj, c_name, c_gender, c_age, c_year]].copy()

    if sel.empty:
        return f"âš ï¸ subject_id={sid} ì— í•´ë‹¹í•˜ëŠ” í™˜ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.", None

    sel = sel.drop_duplicates().reset_index(drop=True)
    sel.columns = ["í™˜ì ë²ˆí˜¸", "í™˜ì ì´ë¦„", "ì„±ë³„", "ë‚˜ì´", "ì¶œìƒë…„ë„"]

    info = f"âœ… í™˜ì ì¡°íšŒ ì™„ë£Œ: subject_id={sid} (í–‰ {len(sel)}ê°œ)"
    return info, sel


# =========================
# 2) í‡´ì›ì¼ ì˜ˆì¸¡
# =========================
@torch.no_grad()
def run_inference(
    subject_id_str: str,
    unified_csv: str,
    ckpt_path: str,
    use_example_sources: bool,
    sources_json_path: str,
    cache_dir: str,
    encode_device_str: str,  # "cpu" | "cuda" | "mps"
    batch_size: int,
):
    # ì…ë ¥ ê²€ì¦
    if not subject_id_str or not subject_id_str.strip().isdigit():
        return "âŒ subject_idëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.", None
    subject_id = int(subject_id_str.strip())

    if not os.path.isfile(unified_csv):
        return f"âŒ unified_csv ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {unified_csv}", None
    if not os.path.isfile(ckpt_path):
        return f"âŒ ì²´í¬í¬ì¸íŠ¸(.pt) ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ckpt_path}", None

    # sources êµ¬ì„±
    if use_example_sources:
        sources = example_sources_config_v3()
    else:
        if not sources_json_path or not os.path.isfile(sources_json_path):
            return "âŒ sources_json_pathê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.", None
        with open(sources_json_path, "r", encoding="utf-8") as f:
            sources = json.load(f)

    # (ì„ íƒ) BRITS CKPT hidden_dim ë¯¸ìŠ¤ë§¤ì¹˜ ë³´ì •(í•„ìš”ì‹œ)
    if "complete_blood_count" in sources:
        sources["complete_blood_count"]["hidden_dim"] = sources["complete_blood_count"].get("hidden_dim", 64)

    # ë‹¨ì¼ DS ìƒì„±(ì „ì²´) â†’ Subsetìœ¼ë¡œ subjectë§Œ ì¶”ì¶œ
    ds_all = HadmTableDatasetV3(
        unified_csv=unified_csv,
        drop_index_death=False,
        drop_30d_postdischarge_death=False,
        crrt_label_csv=None,
        mimic_derived_lods_csv=None,
        mimic_derived_crrt_csv=None,
        crrt_restrict_within_admission_window=True,
        sources=sources,
        encode_device=encode_device_str if encode_device_str in ("cpu","cuda","mps") else "cpu",
        cache_dir=cache_dir if (cache_dir and len(cache_dir.strip())>0) else None,
    )

    idxs = ds_all.df.index[ds_all.df["subject_id"] == subject_id].tolist()
    if len(idxs) == 0:
        return f"âš ï¸ subject_id={subject_id} ì— í•´ë‹¹í•˜ëŠ” HADMì´ ì—†ìŠµë‹ˆë‹¤.", None

    subset = Subset(ds_all, idxs)

    # hadm â†’ (admit, disch)
    tmp = ds_all.df.loc[idxs, ["hadm_id", "admittime", "dischtime"]].copy()
    tmp["admittime"] = pd.to_datetime(tmp["admittime"], errors="coerce")
    tmp["dischtime"] = pd.to_datetime(tmp["dischtime"], errors="coerce")
    times_map = {int(r.hadm_id): (r.admittime, r.dischtime) for r in tmp.itertuples(index=False)}

    # Loader
    loader = DataLoader(
        subset,
        batch_size=max(1, int(batch_size)),
        shuffle=False,
        num_workers=0,
        pin_memory=False,
        collate_fn=collate_hadm_batch_v3,
        drop_last=False,
    )

    # peek â†’ (S,N,E)
    peek = next(iter(loader))
    S = peek["base"].shape[1]
    N = peek["exam_z"].shape[1]
    E = peek["exam_z"].shape[2] if N > 0 else 0

    # ëª¨ë¸ ë¡œë“œ
    device = resolve_device(None)
    model = TableTransformerPredictor(
        num_tables=N, latent_dim=E, base_dim=S,
        d_model=256, nhead=8, depth=3, dim_ff=768,
        dropout=0.15, head_hidden=256,
        use_film=True, use_masked_mean=True
    ).to(device).eval()

    ck = torch.load(ckpt_path, map_location="cpu")
    state_dict = ck.get("model", ck)  # {"model": ...} ë˜ëŠ” state_dict
    model.load_state_dict(state_dict, strict=False)

    # ì¶”ë¡ 
    rows, seen = [], set()
    for batch in loader:
        base = batch["base"].to(device)
        exam_z = batch["exam_z"].to(device)
        exam_mask = batch["exam_mask"].to(device)

        los_pred, readmit_logit = model(base=base, exam_z=exam_z, exam_mask=exam_mask)
        # readmit_prob = torch.sigmoid(readmit_logit)  # í•„ìš”ì‹œ ì‚¬ìš©

        hadm_list = batch["hadm_id"]
        subj_list = batch["subject_id"]
        for i in range(len(hadm_list)):
            hadm = int(hadm_list[i])
            subj = int(subj_list[i])

            lp = float(los_pred[i].item())
            lp_3 = round(lp, 3)  # ì†Œìˆ˜ì  3ìë¦¬

            # ì‹¤ì œ ì…/í‡´ì›ì¼
            admit_dt, disch_true_dt = times_map.get(hadm, (None, None))

            # ì˜ˆì¸¡ í‡´ì›ì¼(LOS 3ìë¦¬ ì‚¬ìš©)
            pred_disch_dt = None
            if admit_dt is not None and pd.notna(admit_dt):
                try:
                    pred_disch_dt = admit_dt + pd.to_timedelta(lp_3, unit="D")
                except Exception:
                    pred_disch_dt = None
            
            # ì˜¤ì°¨(ì¼) â†’ ì†Œìˆ˜ì  3ìë¦¬
            err_days_3 = None
            if (pred_disch_dt is not None) and (disch_true_dt is not None) and pd.notna(disch_true_dt):
                try:
                    err_days = (pred_disch_dt - disch_true_dt).total_seconds() / (24 * 3600.0)
                    err_days_3 = round(err_days, 3)
                except Exception:
                    err_days_3 = None

            key = (subj, hadm)
            if key in seen:
                continue
            seen.add(key)

            rows.append({
                "subject_id": subj,
                "hadm_id": hadm,
                "admittime": admit_dt,
                "dischtime_true": disch_true_dt,
                "pred_dischtime": pred_disch_dt,   # â† lp_3ë¡œ ê³„ì‚°ëœ datetime
                "los_pred_days": lp_3,             # â† ì†Œìˆ˜ì  3ìë¦¬
                "error_days": (float(err_days_3) if err_days_3 is not None else None),  # â† ì†Œìˆ˜ì  3ìë¦¬
            })

    df = pd.DataFrame(
        rows,
        columns=[
            "subject_id", "hadm_id", "admittime", "dischtime_true",
            "pred_dischtime", "los_pred_days", "error_days"
        ],
    ).drop_duplicates().reset_index(drop=True)

    # ë‚ ì§œ ë³´ê¸° ì¢‹ê²Œ
    for c in ["admittime", "dischtime_true", "pred_dischtime"]:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c]).dt.strftime("%Y-%m-%d %H:%M:%S")

    # ìˆ«ì 3ìë¦¬ ë°˜ì˜¬ë¦¼ (í‘œì‹œëŠ” floatë¡œ ìœ ì§€)
    for c in ["los_pred_days", "error_days"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").round(3)

    info = (
        f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: subject_id={subject_id}, HADM {len(df)}ê±´\n"
        f" - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: {os.path.basename(ckpt_path)}\n"
        f" - í…Œì´ë¸” ìˆ˜ N={N}, ì ì¬ ì°¨ì› E={E}, base ì°¨ì› S={S}"
    )
    return info, df


# subject_idë§Œ ë°›ì•„ ì˜ˆì¸¡ ì‹¤í–‰ (ê³ ì • ì¸ì ë˜í•‘)
def run_inference_defaults(subject_id_str: str):
    return run_inference(
        subject_id_str=subject_id_str,
        unified_csv=UNIFIED_CSV,
        ckpt_path=CKPT_PATH,
        use_example_sources=USE_EXAMPLE_SOURCES,
        sources_json_path=SOURCES_JSON_PATH,
        cache_dir=CACHE_DIR,
        encode_device_str=ENCODE_DEVICE,
        batch_size=BATCH_SIZE,
    )


# =========================
# Gradio UI
# =========================
with gr.Blocks(title="UTI í™˜ì ì¡°íšŒ & í‡´ì›ì¼ ì˜ˆì¸¡") as demo:
    gr.Markdown("## ğŸ§‘â€âš•ï¸ í™˜ì ì¡°íšŒ â†’ ğŸ¥ í‡´ì›ì¼ ì˜ˆì¸¡ (subject_id ê¸°ë°˜)\n"
                f"- patients.csv: `{PATIENTS_CSV}`\n"
                f"- ëª¨ë¸: `{CKPT_PATH}`\n"
                f"- ë°ì´í„°: `{UNIFIED_CSV}`\n"
                f"- sources: {'example_sources_config_v3()' if USE_EXAMPLE_SOURCES else SOURCES_JSON_PATH}\n"
                f"- device: `{ENCODE_DEVICE}`, batch_size: {BATCH_SIZE}")

    with gr.Row():
        subject_id_in = gr.Textbox(label="subject_id (ì •ìˆ˜)", placeholder="ì˜ˆ: 10000232")
    with gr.Row():
        patients_csv_in = gr.Textbox(label="patients.csv ê²½ë¡œ", value=PATIENTS_CSV)

    with gr.Row():
        btn_lookup = gr.Button("ğŸ” í™˜ì ì¡°íšŒ")
        btn_predict = gr.Button("ğŸ§® í‡´ì›ì¼ ì˜ˆì¸¡")

    with gr.Row():
        # ì¡°íšŒ ê²°ê³¼
        patient_info_out = gr.Textbox(label="í™˜ì ì¡°íšŒ ìƒíƒœ")
        patient_table_out = gr.Dataframe(label="í™˜ì ì •ë³´", interactive=False)

    with gr.Row():
        # ì˜ˆì¸¡ ê²°ê³¼
        pred_info_out = gr.Textbox(label="ì˜ˆì¸¡ ìƒíƒœ")
        pred_table_out = gr.Dataframe(
            label="ì˜ˆì¸¡ ê²°ê³¼",
            interactive=False
        )
    # ë™ì‘ ë°”ì¸ë”©
    btn_lookup.click(
        fn=lookup_patient,
        inputs=[subject_id_in, patients_csv_in],
        outputs=[patient_info_out, patient_table_out]
    )

    btn_predict.click(
        fn=run_inference_defaults,
        inputs=[subject_id_in],
        outputs=[pred_info_out, pred_table_out]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)
